{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths for each comedian source corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "biburr_path = 'ComedyText/BBText.txt'\n",
    "rpryor_path = 'ComedyText/RPText.txt'\n",
    "ajesel_path = 'ComedyText/AJText.txt'\n",
    "\n",
    "allcom_path = 'ComedyText/AllComText.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bill Burr Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 170333 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(biburr_path, 'rb').read().decode(encoding = \"utf-8\")\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snippet of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill Burr\n",
      "\n",
      "Thank you. Thank you, sir. How are ya? How’s it going? All right. [Cheers and applause] All right, all right, all right, all right. All right, everybody settle down. I wanna get a gun. I do. I really do. I never had that feeling before til\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map characters to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of character to index mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Bill Burr\\n\\nThank you. Thank yo' ---- characters mapped to int ---- > [26 59 62 62  1 26 71 68 68  0  0 43 58 51 64 61  1 75 65 71 11  1 43 58\n",
      " 51 64 61  1 75 65]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:30]), text_as_int[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "B\n",
      "u\n",
      "r\n",
      "r\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Bill Burr\\n\\nThank you. Thank you, sir. How are ya? How’s it going? All right. [Cheers and applause] Al'\n",
      "'l right, all right, all right, all right. All right, everybody settle down. I wanna get a gun. I do. '\n",
      "'I really do. I never had that feeling before till I moved out to Los Angeles. This city just messes w'\n",
      "'ith your mind, you know? It’s overpopulated, technically doesn’t have a water supply. Right? The doll'\n",
      "'ar’s crashing. Shit keeps you up at night. You’re just thinking… “What am I gonna do when the zombies'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Bill Burr\\n\\nThank you. Thank you, sir. How are ya? How’s it going? All right. [Cheers and applause] A'\n",
      "Target data: 'ill Burr\\n\\nThank you. Thank you, sir. How are ya? How’s it going? All right. [Cheers and applause] Al'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 26 ('B')\n",
      "  expected output: 59 ('i')\n",
      "Step    1\n",
      "  input: 59 ('i')\n",
      "  expected output: 62 ('l')\n",
      "Step    2\n",
      "  input: 62 ('l')\n",
      "  expected output: 62 ('l')\n",
      "Step    3\n",
      "  input: 62 ('l')\n",
      "  expected output: 1 (' ')\n",
      "Step    4\n",
      "  input: 1 (' ')\n",
      "  expected output: 26 ('B')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size (number of 100 word sequences to train on)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# How many individual training steps per samples (steps)\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "    rnn2 = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create (and label) model for Bill Burr source text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "biburr_model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 85) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "    example_batch_predictions = biburr_model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           21760     \n",
      "_________________________________________________________________\n",
      "cu_dnngru (CuDNNGRU)         (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 85)            87125     \n",
      "=================================================================\n",
      "Total params: 4,047,189\n",
      "Trainable params: 4,047,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "biburr_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 85)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.443308\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "biburr_model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save checkpoints (i.e. weights) so generative model can be deployed w/o retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './biburr_training'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "26/26 [==============================] - 2s 78ms/step - loss: 4.1671\n",
      "Epoch 2/32\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 2.8493\n",
      "Epoch 3/32\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 2.4428\n",
      "Epoch 4/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 2.2569\n",
      "Epoch 5/32\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 2.1492\n",
      "Epoch 6/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 2.0551\n",
      "Epoch 7/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 1.9667\n",
      "Epoch 8/32\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 1.8837\n",
      "Epoch 9/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 1.8113\n",
      "Epoch 10/32\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 1.7446\n",
      "Epoch 11/32\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 1.6851\n",
      "Epoch 12/32\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 1.6283\n",
      "Epoch 13/32\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 1.5764\n",
      "Epoch 14/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 1.5270\n",
      "Epoch 15/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 1.4822\n",
      "Epoch 16/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 1.4375\n",
      "Epoch 17/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 1.3962\n",
      "Epoch 18/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 1.3571\n",
      "Epoch 19/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 1.3171\n",
      "Epoch 20/32\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 1.2790\n",
      "Epoch 21/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 1.2443\n",
      "Epoch 22/32\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 1.2062\n",
      "Epoch 23/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 1.1717\n",
      "Epoch 24/32\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 1.1359\n",
      "Epoch 25/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 1.0992\n",
      "Epoch 26/32\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 1.0620\n",
      "Epoch 27/32\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 1.0231\n",
      "Epoch 28/32\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.9872\n",
      "Epoch 29/32\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.9511\n",
      "Epoch 30/32\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.9108\n",
      "Epoch 31/32\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.8733\n",
      "Epoch 32/32\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.8340\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 32\n",
    "history = biburr_model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./biburr_training/ckpt_32'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "biburr_model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "biburr_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "biburr_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            21760     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 85)             87125     \n",
      "=================================================================\n",
      "Total params: 4,047,189\n",
      "Trainable params: 4,047,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "biburr_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string,temperature=0.5):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "    num_generate = 1500\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  #  temperature = .50\n",
    "\n",
    "  # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "      # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey How did a start thinking of shit, but I’m not wanted to go to story. I wanna be something. “How did it going?” “I think I’m gonna go out and get the balls to have a kid, I’ll suck your dick white guy thinking, “I have a kid, I’m like, “What do I go, “I’m not saying that were started talking about it. I think it’s a lot of it, it’s a good thing about it, you know? I just don’t know they had hands. They’re just fucking gonna have a kid, what do you do now? You can’t get married, and then the most difficult job on the planet. Oh, it’s the greatest thing ever. Big me us the most difficult job on the planet. Oh, it’s a bunch of claps, right? You got the balls to get in something, you know? It’s like, “why would you really had to do what kills me, what do you were said it looked to make her there was something like, “What do you do me a face-lifts? ‘Cause right there. I don’t know what happened? I love and shit. I go, “Well, that’s not a pit bull. It’s so a water span in the back of a woman’s a good is at the matt happened. They’re just gonna go out and say a little bit of ic! It’s not a little childish, you got to be honest with your day with this dog to be gonna lie to your life is awesome. It’s a planet. I didn’t get it. I think I look at the couch. “Hey, Breaking out of the most difficult job on the place, you know? I got the balls to make sense of a fucking sick of shit how they fall? Oh, my god. I’ll give you one. All right? But he’s all excited. She goes, “Well, I don’t giv\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(biburr_model, start_string=u\"Hey\",temperature=.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Richard Pryor Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, except functions are already defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 102554 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(rpryor_path, 'rb').read().decode(encoding = \"utf-8\")\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Richard Pryor\n",
      "\n",
      "We are gathered here today… to make sure… everyone eats. If not each other… food. I was gonna talk about something that’s very serious… and I hope no one gets offended. I wanna talk about fucking. And sometimes I talk about it. And a l\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R\n",
      "i\n",
      "c\n",
      "h\n",
      "a\n",
      "r\n",
      "d\n",
      " \n",
      "P\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Richard Pryor\\n\\nWe are gathered here today… to make sure… everyone eats. If not each other… food. I wa'\n",
      "'s gonna talk about something that’s very serious… and I hope no one gets offended. I wanna talk about'\n",
      "' fucking. And sometimes I talk about it. And a lot of people in the audience… don’t know what I mean.'\n",
      "' So would you raise your hand it you don’t know what fucking is… so we can watch your ass when you le'\n",
      "'ave here? Because not enough fuckin’… goin’ on in America. Americans. Reagan get in, you stop fuckin’'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Richard Pryor\\n\\nWe are gathered here today… to make sure… everyone eats. If not each other… food. I w'\n",
      "Target data: 'ichard Pryor\\n\\nWe are gathered here today… to make sure… everyone eats. If not each other… food. I wa'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size (number of 100 word sequences to train on)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# How many individual training steps per samples (steps)\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "    rnn2 = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpryor_model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 76) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "    example_batch_predictions = rpryor_model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (64, None, 256)           19456     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_4 (CuDNNGRU)       (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (64, None, 76)            77900     \n",
      "=================================================================\n",
      "Total params: 4,035,660\n",
      "Trainable params: 4,035,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rpryor_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 76)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.3299656\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpryor_model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './rpryor_training'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 4.7946\n",
      "Epoch 2/32\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 3.3523\n",
      "Epoch 3/32\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 2.8040\n",
      "Epoch 4/32\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 2.4929\n",
      "Epoch 5/32\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 2.3081\n",
      "Epoch 6/32\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 2.2118\n",
      "Epoch 7/32\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 2.1282\n",
      "Epoch 8/32\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 2.0742\n",
      "Epoch 9/32\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 1.9961\n",
      "Epoch 10/32\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 1.9459\n",
      "Epoch 11/32\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 1.8833\n",
      "Epoch 12/32\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 1.8181\n",
      "Epoch 13/32\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 1.7623\n",
      "Epoch 14/32\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 1.7136\n",
      "Epoch 15/32\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 1.6593\n",
      "Epoch 16/32\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 1.6129\n",
      "Epoch 17/32\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 1.5671\n",
      "Epoch 18/32\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 1.5267\n",
      "Epoch 19/32\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 1.4910\n",
      "Epoch 20/32\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 1.4466\n",
      "Epoch 21/32\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 1.4060\n",
      "Epoch 22/32\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 1.3682\n",
      "Epoch 23/32\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 1.3392\n",
      "Epoch 24/32\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 1.2975\n",
      "Epoch 25/32\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 1.2715\n",
      "Epoch 26/32\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 1.2359\n",
      "Epoch 27/32\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 1.1973\n",
      "Epoch 28/32\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 1.1617\n",
      "Epoch 29/32\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 1.1334\n",
      "Epoch 30/32\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 1.0973\n",
      "Epoch 31/32\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 1.0661\n",
      "Epoch 32/32\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 1.0297\n"
     ]
    }
   ],
   "source": [
    "history = rpryor_model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./rpryor_training/ckpt_32'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpryor_model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "rpryor_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "rpryor_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey’s gonna get off the talked in the toilet in the toing to me. But he was going to help. See that shit is the motherfuckers didn’t want to we the street about it. I say. “I’m gonna be fucking were. We got a lot of shit. I was in the toilet. And you fuck all the shit going. And you have to see the fuck around. Just a minute. Whought it the crubs was on the time. You know what I’m going to hel. Shit. Shit. Care this but the lions some of the bolks and shit. I’m gonna do. They got the brothers was some shit that shit in the corner… I’m gonna get no more. I was a little on your life. Motherfucker called to me. I’m gonna talk about friends and shit, man. I’m gonna be out there wasn’t no more. I ain’t gonna be in your now. You know, to your mick hanged back. I was a black come in your when you doin’? Shit, are you been to the motherfuckers doing in the time, you say, ”We’re not gonna people in the tried to say. “Fuck it. So I can’t even look like a walk about the motherfuckers done fucking with the shit the there weeks a been and shit. You know. That’s a little on your four down here. So you see a little kidda can of the borning. They go away from the tool the country then they can’t come to the the shit to me too lawyers and shit, like when the motherfuckers was one of them motherfuckers or nothin’ to some here. You know. I was out there and said. “You’re just smokin’ a black motherfucker. I’m gonna go home. I don’t know what it do that shit in the toilet. I mean, no cheetah!” “Wha\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(rpryor_model, start_string=u\"Hey\",temperature=.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthony Jeselnik Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 93327 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(ajesel_path, 'rb').read().decode(encoding = \"utf-8\")\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthony Jeselnik\n",
      "\n",
      "Now… my best friend’s wife is a born again Christian, and we do not get along at all. The other day, she called me up to yell at me, saying I’m a terrible influence on her husband because he called her a bad name. I said, “What? Did\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "n\n",
      "t\n",
      "h\n",
      "o\n",
      "n\n",
      "y\n",
      " \n",
      "J\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Anthony Jeselnik\\n\\nNow… my best friend’s wife is a born again Christian, and we do not get along at al'\n",
      "'l. The other day, she called me up to yell at me, saying I’m a terrible influence on her husband beca'\n",
      "'use he called her a bad name. I said, “What? Did he call you a bitch?” She said, “No, Anthony. He did'\n",
      "' not use the B word.” I said, “Uh-oh.” “Did he call you a cunt?” She said, “No.” I said, “Well, then '\n",
      "'he didn’t hear it from me.” Yeah, that’s pretty much the greatest opening joke of all time. Because e'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Anthony Jeselnik\\n\\nNow… my best friend’s wife is a born again Christian, and we do not get along at a'\n",
      "Target data: 'nthony Jeselnik\\n\\nNow… my best friend’s wife is a born again Christian, and we do not get along at al'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size (number of 100 word sequences to train on)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# How many individual training steps per samples (steps)\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "    rnn2 = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajesel_model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 82) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "    example_batch_predictions = ajesel_model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (64, None, 256)           20992     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_6 (CuDNNGRU)       (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (64, None, 82)            84050     \n",
      "=================================================================\n",
      "Total params: 4,043,346\n",
      "Trainable params: 4,043,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ajesel_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajesel_model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './ajesel_training'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "14/14 [==============================] - 2s 137ms/step - loss: 4.5759\n",
      "Epoch 2/32\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 3.4569\n",
      "Epoch 3/32\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 2.9745\n",
      "Epoch 4/32\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 2.7312\n",
      "Epoch 5/32\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 2.5374\n",
      "Epoch 6/32\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 2.4222\n",
      "Epoch 7/32\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 2.3416\n",
      "Epoch 8/32\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 2.2777\n",
      "Epoch 9/32\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 2.2208\n",
      "Epoch 10/32\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 2.1641\n",
      "Epoch 11/32\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 2.1086\n",
      "Epoch 12/32\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 2.0529\n",
      "Epoch 13/32\n",
      "14/14 [==============================] - 1s 65ms/step - loss: 1.9984\n",
      "Epoch 14/32\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 1.9451\n",
      "Epoch 15/32\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 1.8952\n",
      "Epoch 16/32\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 1.8476\n",
      "Epoch 17/32\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 1.8005\n",
      "Epoch 18/32\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 1.7535\n",
      "Epoch 19/32\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 1.7076\n",
      "Epoch 20/32\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 1.6629\n",
      "Epoch 21/32\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 1.6194\n",
      "Epoch 22/32\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 1.5768\n",
      "Epoch 23/32\n",
      "14/14 [==============================] - 1s 56ms/step - loss: 1.5390\n",
      "Epoch 24/32\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 1.4954\n",
      "Epoch 25/32\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 1.4567\n",
      "Epoch 26/32\n",
      "14/14 [==============================] - 1s 61ms/step - loss: 1.4155\n",
      "Epoch 27/32\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 1.3769\n",
      "Epoch 28/32\n",
      "14/14 [==============================] - 1s 56ms/step - loss: 1.3395\n",
      "Epoch 29/32\n",
      "14/14 [==============================] - 1s 58ms/step - loss: 1.3000\n",
      "Epoch 30/32\n",
      "14/14 [==============================] - 1s 59ms/step - loss: 1.2560\n",
      "Epoch 31/32\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 1.2183\n",
      "Epoch 32/32\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 1.1811\n"
     ]
    }
   ],
   "source": [
    "history = ajesel_model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ajesel_training/ckpt_32'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ajesel_model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "ajesel_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "ajesel_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sould not me and say, \"Anthony, who I was like,\n",
      "\"Anthony,\n",
      "it was where she had tre possont of the was.\n",
      "I come time I am a couple of my family for over to the hotech at abortion couldn’t been you a great friends who started comedian she. I might now that she’m gut up aftrongh, a packyous.\n",
      "You are not a right before you go to me.\n",
      "Ald the ather a blind jokes\n",
      "I accused poon, you’re even comedian. [ Laughter ] And people wime lase.\n",
      "That joke to start to text me.\n",
      "You should all me.\n",
      "That's a forgut because I get thought it. But numblece. Soundry best everything is a surcrucime\n",
      "her wathing to her one me on me.\n",
      "I never getting\n",
      "on the war to the more of a time of me her ones good morey if it had a bad here. So you know? He was a bad.\n",
      "I could ’t a fucking tried to dead. I said, “Wo mon up and phenemally pees hisco.\n",
      "That the only goter, you know. But I mean, she dad she was in fron minded with pen of and tele is a lot get.\n",
      "Like, the exactid about it.\n",
      "And they really more in the worst probare done.\n",
      "\n",
      "Anthony, that joke, who know what the got reading a bad peing, but I mean, she toughed a good friend of a tomither your house.\n",
      "Everyone is bed in to tell you. But the door. ‘Cause I worry about her out me that she face is to the doors. It’s awam.\n",
      "I hours ago, that joke is that as I have one really a couple a gied her a dight because the not a boby and great friend of your money.\n",
      "You're friend Jese. And I said, \"Goked me as the second the Buss.\n",
      "And they say it. Like, a blind\n",
      "and perusiat.” [ Laug\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(ajesel_model, start_string=u\"So\",temperature=.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Transcripts Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 628637 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(allcom_path, 'rb').read().decode(encoding = \"utf-8\")\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\n",
      "o\n",
      "w\n",
      "…\n",
      " \n",
      "m\n",
      "y\n",
      " \n",
      "b\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Now… my best friend’s wife is a born again Christian, and we do not get along at all. The other day, '\n",
      "'she called me up to yell at me, saying I’m a terrible influence on her husband because he called her '\n",
      "'a bad name. I said, “What? Did he call you a bitch?” She said, “No, Anthony. He did not use the B wor'\n",
      "'d.” I said, “Uh-oh.” “Did he call you a cunt?” She said, “No.” I said, “Well, then he didn’t hear it '\n",
      "'from me.” Yeah, that’s pretty much the greatest opening joke of all time. Because even if you’ve neve'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Now… my best friend’s wife is a born again Christian, and we do not get along at all. The other day,'\n",
      "Target data: 'ow… my best friend’s wife is a born again Christian, and we do not get along at all. The other day, '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size (number of 100 word sequences to train on)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# How many individual training steps per samples (steps)\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "    rnn2 = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcom_model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 93) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "    example_batch_predictions = allcom_model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (64, None, 256)           23808     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_8 (CuDNNGRU)       (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (64, None, 93)            95325     \n",
      "=================================================================\n",
      "Total params: 4,057,437\n",
      "Trainable params: 4,057,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "allcom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcom_model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './allcom_training'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "98/98 [==============================] - 6s 61ms/step - loss: 2.7904\n",
      "Epoch 2/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 2.0252\n",
      "Epoch 3/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 1.7447\n",
      "Epoch 4/32\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 1.5759\n",
      "Epoch 5/32\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 1.4613\n",
      "Epoch 6/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 1.3834\n",
      "Epoch 7/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 1.3209\n",
      "Epoch 8/32\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 1.2681\n",
      "Epoch 9/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 1.2218\n",
      "Epoch 10/32\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 1.1796\n",
      "Epoch 11/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 1.1392\n",
      "Epoch 12/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 1.0982\n",
      "Epoch 13/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 1.0589\n",
      "Epoch 14/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 1.0167\n",
      "Epoch 15/32\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 0.9780\n",
      "Epoch 16/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 0.9357\n",
      "Epoch 17/32\n",
      "98/98 [==============================] - 4s 45ms/step - loss: 0.8938\n",
      "Epoch 18/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 0.8524\n",
      "Epoch 19/32\n",
      "98/98 [==============================] - 4s 42ms/step - loss: 0.8103\n",
      "Epoch 20/32\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 0.7707\n",
      "Epoch 21/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 0.7287\n",
      "Epoch 22/32\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 0.6914\n",
      "Epoch 23/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 0.6575\n",
      "Epoch 24/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 0.6264\n",
      "Epoch 25/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 0.5969\n",
      "Epoch 26/32\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 0.5685\n",
      "Epoch 27/32\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 0.5474\n",
      "Epoch 28/32\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 0.5251\n",
      "Epoch 29/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 0.5080\n",
      "Epoch 30/32\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 0.4938\n",
      "Epoch 31/32\n",
      "98/98 [==============================] - 4s 43ms/step - loss: 0.4806\n",
      "Epoch 32/32\n",
      "98/98 [==============================] - 4s 44ms/step - loss: 0.4657\n"
     ]
    }
   ],
   "source": [
    "history = allcom_model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./allcom_training/ckpt_32'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcom_model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "allcom_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "allcom_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So the court. Like the thing today,\n",
      "the thing that the cops come over.” – “Fuck Jim Brown.” “Oh, my God. What, are you gonna be great. Come here, like…” He really was, like, trying to tell me. I know what you say that stuff. The opposite of tender. I have never seen a baby. What the fuck am I, right? Ha. Does it happen? Did you not seen him watching it. Probably have a great life, but it’s good. It’s… you know, if I’m a bunch of cripit of the country was like, “Hey, man, you got to be the best thing about it. Even when I watch sports. I didn’t have anything to tell them this shit work. Hey, you don’t want to do that. My girl was all about it. He’s like, “No, it’s fucking his back, and he couldn’t rind of grape drink at my friend’s house… like a… “I want this shit to stop.” Crackheads are like, “No, I’m sorry, ladies. I just feel like doing anything around the corner to see the shit out of the dog that way. Like, the other day, she got her hair cut. Two inches trimmed off of her hair. Then should be the end of a date, I understand hitting a woman’s at the next table like… You know. That’s what it is. I think it’s irn on the planet. She’s a mother.” And continues on, and immediately, I just like… The next move is a bitch. It’s hard enough just to walk through life decent. As a person. But he died in that room today. Laughing. ‘Cause you don’t know what the fuck I ever got married for. 13 goddamn years of this guy. I just feel like a town square fool. There should be a model the w\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(allcom_model, start_string=u\"So\",temperature=.35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
